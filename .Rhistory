import numpy as np
import matplotlib.pyplot as plt
from scipy.odr import ODR, Model, RealData
from sklearn.linear_model import LinearRegression
# Génération de données avec erreurs
np.random.seed(0)
x = np.linspace(0, 10, 10)
y = 2 * x + 1 + np.random.normal(size=x.size)
# Régression MCO
mco_model = LinearRegression()
mco_model.fit(x.reshape(-1, 1), y)
y_mco_pred = mco_model.predict(x.reshape(-1, 1))
# Régression Orthogonale
def orthogonal_func(beta, x):
return beta[0] * x + beta[1]
data = RealData(x, y)
model = Model(orthogonal_func)
odr = ODR(data, model, beta0=[1., 2.])
odr_res = odr.run()
y_ortho_pred = orthogonal_func(odr_res.beta, x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_mco_pred, label='MCO', color='red')
plt.plot(x, y_ortho_pred, label='Régression Orthogonale', color='green')
plt.legend()
plt.show()
#Splines et Modèles Additifs Généralisés (GAM)
from patsy import dmatrix
from statsmodels.api import OLS
# Génération de splines
x_basis = dmatrix("bs(x, df=6)", {"x": x}, return_type='dataframe')
model = OLS(y, x_basis).fit()
y_spline_pred = model.predict(x_basis)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_spline_pred, color='purple', label='Splines')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
# Génération de données aléatoires pour l'exemple
np.random.seed(0)
x = 2 * np.random.rand(100, 1)
y = 4 + 3 * x + np.random.randn(100, 1)
# Ajustement du modèle de régression linéaire
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)
# Affichage des coefficients
print(f"Ordonnée à l'origine: {model.intercept_[0]}")
print(f"Coefficient (pente): {model.coef_[0][0]}")
# Tracer les données et la droite de régression
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Droite de régression')
# Tracer les écarts (résidus)
for i in range(len(x)):
plt.plot([x[i], x[i]], [y[i], y_pred[i]], color='gray', linestyle='--')
# Ajouter les légendes et afficher le graphique
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Régression linéaire par les moindres carrés ordinaires')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from scipy.odr import ODR, Model, Data, RealData
# Génération de données aléatoires pour l'exemple
np.random.seed(0)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)
# Fonction de modèle linéaire
def linear_model(B, x):
return B[0] * x + B[1]
# Création des données pour ODR
data = RealData(x, y)
# Définition du modèle ODR
model = Model(linear_model)
# Estimation initiale des paramètres
beta0 = [1., 1.]
# Création de l'instance ODR
odr = ODR(data, model, beta0=beta0)
# Ajustement du modèle
output = odr.run()
# Paramètres ajustés
beta_hat = output.beta
print(f"Coefficient (pente): {beta_hat[0]}")
print(f"Ordonnée à l'origine: {beta_hat[1]}")
# Prédictions
y_pred = linear_model(beta_hat, x)
# Tracer les données et la droite de régression orthogonale
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Droite de régression orthogonale')
# Tracer les écarts (résidus orthogonaux)
for i in range(len(x)):
# Projection orthogonale sur la droite de régression
x_proj = (x[i] + beta_hat[0] * (y[i] - beta_hat[1])) / (1 + beta_hat[0]**2)
y_proj = linear_model(beta_hat, x_proj)
plt.plot([x[i], x_proj], [y[i], y_proj], color='gray', linestyle='--')
# Ajouter les légendes et afficher le graphique
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Régression orthogonale par les moindres carrés orthogonaux')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from scipy.odr import ODR, Model, RealData
from sklearn.linear_model import LinearRegression
# Génération de données avec erreurs
np.random.seed(0)
x = np.linspace(0, 10, 10)
y = 2 * x + 1 + np.random.normal(size=x.size)
# Régression MCO
mco_model = LinearRegression()
mco_model.fit(x.reshape(-1, 1), y)
y_mco_pred = mco_model.predict(x.reshape(-1, 1))
# Régression Orthogonale
def orthogonal_func(beta, x):
return beta[0] * x + beta[1]
data = RealData(x, y)
model = Model(orthogonal_func)
odr = ODR(data, model, beta0=[1., 2.])
odr_res = odr.run()
y_ortho_pred = orthogonal_func(odr_res.beta, x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_mco_pred, label='MCO', color='red')
plt.plot(x, y_ortho_pred, label='Régression Orthogonale', color='green')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity
# Génération de données
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(x).ravel()
# Ajout de bruit
y[::5] += 3 * (0.5 - np.random.rand(16))
# Régression par noyau
from statsmodels.nonparametric.kernel_regression import KernelReg
kr = KernelReg(y, x, 'c')
y_pred, _ = kr.fit(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_pred, color='red', label='Régression par Noyau')
plt.legend()
plt.show()
from sklearn.neighbors import KNeighborsRegressor
# Régression KNN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(x, y)
y_knn_pred = knn.predict(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_knn_pred, color='green', label='Régression KNN')
plt.legend()
plt.show()
#Splines et Modèles Additifs Généralisés (GAM)
from patsy import dmatrix
from statsmodels.api import OLS
# Génération de splines
x_basis = dmatrix("bs(x, df=6)", {"x": x}, return_type='dataframe')
model = OLS(y, x_basis).fit()
y_spline_pred = model.predict(x_basis)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_spline_pred, color='purple', label='Splines')
plt.legend()
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
# Génération de données aléatoires pour l'exemple
np.random.seed(0)
x = np.sort(5 * np.random.rand(100))
y = np.sin(x) + 0.3 * np.random.randn(100)
# Définition de la fonction de noyau gaussien pour la régression par noyau
def kernel_regression(x, y, bandwidth=0.5):
kde = gaussian_kde(x, bw_method=bandwidth / np.std(x, ddof=1))
y_pred = np.zeros_like(x)
for i in range(len(x)):
weights = kde(x - x[i])
y_pred[i] = np.sum(weights * y) / np.sum(weights)
return y_pred
# Estimation des valeurs prédites avec la régression par noyau
bandwidth = 0.5
y_pred = kernel_regression(x, y, bandwidth)
# Tracé des données et de la courbe de régression par noyau
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Régression par noyau')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title(f"Régression par noyau avec une bande passante de {bandwidth}")
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
# Génération de données aléatoires pour l'exemple
np.random.seed(0)
x = np.sort(5 * np.random.rand(100))
y = np.sin(x) + 0.3 * np.random.randn(100)
# Définition de la fonction de noyau gaussien pour la régression par noyau
def kernel_regression(x, y, bandwidth=0.5):
kde = gaussian_kde(x, bw_method=bandwidth / np.std(x, ddof=1))
y_pred = np.zeros_like(x)
for i in range(len(x)):
weights = kde(x - x[i])
y_pred[i] = np.sum(weights * y) / np.sum(weights)
return y_pred
# Estimation des valeurs prédites avec la régression par noyau
bandwidth = 0.5
y_pred = kernel_regression(x, y, bandwidth)
# Tracé des données et de la courbe de régression par noyau
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Régression par noyau')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title(f"Régression par noyau avec une bande passante de {bandwidth}")
plt.show()
#Splines et Modèles Additifs Généralisés (GAM)
from patsy import dmatrix
from statsmodels.api import OLS
# Génération de splines
x_basis = dmatrix("bs(x, df=6)", {"x": x}, return_type='dataframe')
model = OLS(y, x_basis).fit()
y_spline_pred = model.predict(x_basis)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_spline_pred, color='purple', label='Splines')
plt.legend()
plt.title('Modèle Additif Généralisé')
plt.show()
from sklearn.ensemble import RandomForestRegressor
# Régression Random Forest
rf = RandomForestRegressor(n_estimators=100)
rf.fit(x, y)
y_rf_pred = rf.predict(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_rf_pred, color='orange', label='Forêt Aléatoire')
plt.legend()
plt.title('Forêt aléatoire')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
# Génération de données synthétiques
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = 2 * x**2 - 3 * x + 1 + np.random.normal(0, 1, x.shape)
# Modèle Linéaire Simple
lin_reg = LinearRegression()
lin_reg.fit(x, y)
y_lin_pred = lin_reg.predict(x)
# Modèle Polynomial (de degré 2)
poly = PolynomialFeatures(degree=2)
x_poly = poly.fit_transform(x)
poly_reg = LinearRegression()
poly_reg.fit(x_poly, y)
y_poly_pred = poly_reg.predict(x_poly)
# Modèle par Forêt Aléatoire
rf = RandomForestRegressor(n_estimators=100)
rf.fit(x, y.ravel())
y_rf_pred = rf.predict(x)
# Visualisation des résultats
plt.scatter(x, y, label='Données')
plt.plot(x, y_lin_pred, label='Modèle Linéaire', color='red')
plt.plot(x, y_poly_pred, label='Modèle Polynomial (degré 2)', color='green')
plt.plot(x, y_rf_pred, label='Forêt Aléatoire', color='orange')
plt.legend()
plt.show()
from sklearn.neighbors import KNeighborsRegressor
# Régression KNN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(x, y)
y_knn_pred = knn.predict(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_knn_pred, color='green', label='Régression KNN')
plt.legend()
plt.title('k-NN')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity
# Génération de données
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(x).ravel()
# Ajout de bruit
y[::5] += 3 * (0.5 - np.random.rand(16))
# Régression par noyau
from statsmodels.nonparametric.kernel_regression import KernelReg
kr = KernelReg(y, x, 'c')
y_pred, _ = kr.fit(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_pred, color='red', label='Régression par Noyau')
plt.legend()
plt.title('Régression par noyau')
plt.show()
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity
# Génération de données
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(x).ravel()
# Ajout de bruit
y[::5] += 3 * (0.5 - np.random.rand(16))
# Régression par noyau
from statsmodels.nonparametric.kernel_regression import KernelReg
kr = KernelReg(y, x, 'c')
y_pred, _ = kr.fit(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_pred, color='red', label='Régression par Noyau')
plt.legend()
plt.title('Régression par noyau')
plt.show()
plt.close()
View(y_pred)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity
# Génération de données
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(x).ravel()
# Ajout de bruit
y[::5] += 3 * (0.5 - np.random.rand(16))
# Régression par noyau
from statsmodels.nonparametric.kernel_regression import KernelReg
kr = KernelReg(y, x, 'c')
y_pred, _ = kr.fit(x)
# Visualisation
plt.scatter(x, y, label='Données')
plt.plot(x, y_pred, color='red', label='Régression par Noyau')
plt.legend()
plt.title('Régression par noyau')
plt.show()
plt.close()
View(y_poly_pred)
