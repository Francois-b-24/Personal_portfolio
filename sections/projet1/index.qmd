---
title: "M√©thodes de pr√©vision"
title-block-banner: true
toc: true
toc-title: üìö Table des mati√®res
lang: fr
number-sections: true
subtitle: ""
author: "`BOUSSENGUI Fran√ßois` - `BARRE Nicolas` - `SABAYE Fried` "
image: /assets/img/projet1/eco.jpg
description: "*Travaux de recherche en √©conom√©trie*"
categories: [√âconom√©trie]
format:
    html:
        code-fold: true
        code-line-numbers: false
        anchor-sections: true
        smooth-scroll: true
        citations-hover: true
        footnotes-hover: true
        html-math-method: mathjax
---

::: {.tech-badges}
::: {.tech-badge .badge-r}
{{< fa brands r-project >}} R
:::
::: {.tech-badge .badge-python}
{{< fa brands python >}} Python
:::
::: {.tech-badge .badge-econometrie}
{{< fa chart-line >}} √âconom√©trie
:::
::: {.tech-badge .badge-ml}
{{< fa brain >}} Machine Learning
:::
:::

```{r echo=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.align = "center", out.width = "100%")
```

```{r echo=FALSE}
library(reticulate)
use_virtualenv("/Users/f.b/.virtualenvs/r-reticulate")
```

# Contexte 

Les **Moindres Carr√©s Ordinaires** (MCO), ou en anglais Ordinary Least Squares (OLS) est une m√©thode statistique utilis√©e pour mod√©liser la relation entre deux variables quantitatives. Il s'agit de la forme la plus √©l√©mentaire de la r√©gression lin√©aire. En r√©gression lin√©aire simple, une variable ind√©pendante (ou explicative) $x$, est utilis√©e pour pr√©dire une variable d√©pendante (ou r√©ponse) $y$. L'objectif de la r√©gression lin√©aire simple est de trouver les valeurs des coefficients $\beta_0$ et $\beta_1$ qui minimisent la somme des carr√©s des erreurs entre les valeurs observ√©es et les valeurs pr√©dites par le mod√®le.

L'√©conom√©trie offre un panel de m√©thodes de pr√©vision statistique qui d√©pendent de la nature des donn√©es ainsi que de la probl√©matique √† r√©soudre.

Ainsi, ces r√©sultats s'inscrivent dans le cadre de travaux de recherche en √©conom√©trie r√©alis√©es durant ma seconde ann√©e de Master en √©conomie.

# Objectifs 

Les objectifs de ces travaux sont divers.

Principalement, ces travaux portent sur la pr√©sentation des principales m√©thodes alternatives aux **Moindres Carr√©s Ordinaires** (MCO).

Dans un premier temps, il √©t√© question de proposer une repr√©sentation alternatives des principaux r√©sultats d'une r√©gression lin√©aire simple. Pour ce faire, nous avons propos√© une repr√©sentation sous forme *diagrammatique* (graphique/visuel) des principaux r√©sultats de la m√©thode des **Moindres Carr√©s Ordinaires**.

Puis, dans un second temps, il a √©t√© question de pr√©senter et de classifier les principales m√©thodes alternatives √† celles des **Moindres Carr√©s Ordinaires**.

# M√©thodologie

Pour atteindre ces objectifs, nous avons analyser des travaux de recherche en √©conom√©trie et nous avons travaill√© en √©troite collaboration avec notre professeur r√©f√©rent en la mati√®re.

Ainsi, nos recherches nous ont permis d'aboutir √† la proposition d'une repr√©sentation graphique des principaux r√©sultats d'une r√©gression lin√©aire simple dont la sp√©cification math√©matiques est : $y = \beta_0 + \beta_1x + \epsilon$, o√π :

-   $y$ est la variable d√©pendante (la r√©ponse)
-   $x$ est la variable explicative (le pr√©dicteur)
-   $\beta_0$ est l'ordonn√©e √† l'origine (l'intercept)
-   $\beta_1$ est le coefficient de regression (la pente)
-   $\epsilon$ est l'erreur al√©atoire

Par ailleurs, nous avons mis en exergue plusieurs familles de m√©thodes de pr√©vision. Nous pouvons les classer comme suit :

1.  Les m√©thodes d'estimation d√©crivant une **relation lin√©aire** entre la variable √† expliquer $y$ et la/les variable(s) explicative(s) $X$.

-   **Moindres carr√©s ordinaires** : C'est une m√©thode utilis√©e pour estimer les param√®tres d'un mod√®le de r√©gression lin√©aire. Elle minimise la somme des carr√©s des diff√©rences entre les valeurs observ√©es et les valeurs pr√©dites par le mod√®le.

::: {.callout-tip icon="false" collapse="true"}
## NB

En d'autres termes, l'objectif est de minimiser la fonction de co√ªt suivante : $S(a,b) = \sum_{i=1}^{n} (y_i - (a + bx_i ))^2$.
:::

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# G√©n√©ration de donn√©es al√©atoires pour l'exemple
np.random.seed(0)
x = 2 * np.random.rand(100, 1)
y = 4 + 3 * x + np.random.randn(100, 1)

# Ajustement du mod√®le de r√©gression lin√©aire
model = LinearRegression()
model.fit(x, y)
y_pred = model.predict(x)

# Affichage des coefficients
print(f"Ordonn√©e √† l'origine: {model.intercept_[0]}")
print(f"Coefficient (pente): {model.coef_[0][0]}")

# Tracer les donn√©es et la droite de r√©gression
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Droite de r√©gression')

# Tracer les √©carts (r√©sidus)
for i in range(len(x)):
    plt.plot([x[i], x[i]], [y[i], y_pred[i]], color='gray', linestyle='--')

# Ajouter les l√©gendes et afficher le graphique
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('R√©gression lin√©aire par les moindres carr√©s ordinaires')
plt.show()

```

-   **R√©gression orthogonale** : √âgalement connue sous le nom de r√©gression des **Moindres Carr√©s Orthogonaux** (Orthogonal Least Squares Regression) ou r√©gression de Deming, est une m√©thode qui prend en compte les erreurs dans les deux variables, √† la fois la variable ind√©pendante ($x$) et la variable d√©pendante ($y$). Contrairement √† la r√©gression lin√©aire ordinaire qui minimise uniquement les erreurs en $y$, la r√©gression orthogonale minimise la somme des distances orthogonales entre les points de donn√©es et la ligne de r√©gression.

::: {.callout-tip icon="false" collapse="true"}
## NB

En effet, l'erreur orthogonale pour chaque point $(x_i, y_i)$ est d√©fini√© comme la distance perpendiculaire de ce point √† la ligne $(y = a + bx)$.

Cette distance peut √™tre exprim√© par la formule : $d_i = \frac{|bx_i - y_i + a|}{1 + b^2}$.

La somme des carr√©s des distances orthogonales est donc :

$S(a,b) = \sum_{i=1}^{n} \frac{(bx_i + y_i +a)^2}{1+b^2}$.
:::

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from scipy.odr import ODR, Model, Data, RealData

# G√©n√©ration de donn√©es al√©atoires pour l'exemple
np.random.seed(0)
x = 2 * np.random.rand(100)
y = 4 + 3 * x + np.random.randn(100)

# Fonction de mod√®le lin√©aire
def linear_model(B, x):
    return B[0] * x + B[1]

# Cr√©ation des donn√©es pour ODR
data = RealData(x, y)

# D√©finition du mod√®le ODR
model = Model(linear_model)

# Estimation initiale des param√®tres
beta0 = [1., 1.]

# Cr√©ation de l'instance ODR
odr = ODR(data, model, beta0=beta0)

# Ajustement du mod√®le
output = odr.run()

# Param√®tres ajust√©s
beta_hat = output.beta
print(f"Coefficient (pente): {beta_hat[0]}")
print(f"Ordonn√©e √† l'origine: {beta_hat[1]}")

# Pr√©dictions
y_pred = linear_model(beta_hat, x)

# Tracer les donn√©es et la droite de r√©gression orthogonale
plt.scatter(x, y, color='blue', label='Observations')
plt.plot(x, y_pred, color='red', label='Droite de r√©gression orthogonale')

# Tracer les √©carts (r√©sidus orthogonaux)
for i in range(len(x)):
    # Projection orthogonale sur la droite de r√©gression
    x_proj = (x[i] + beta_hat[0] * (y[i] - beta_hat[1])) / (1 + beta_hat[0]**2)
    y_proj = linear_model(beta_hat, x_proj)
    plt.plot([x[i], x_proj], [y[i], y_proj], color='gray', linestyle='--')

# Ajouter les l√©gendes et afficher le graphique
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('R√©gression orthogonale par les moindres carr√©s orthogonaux')
plt.show()
```

-   **Moindre Carr√©s Ordinaires VS R√©gression orthogonale**

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from scipy.odr import ODR, Model, RealData
from sklearn.linear_model import LinearRegression

# G√©n√©ration de donn√©es avec erreurs
np.random.seed(0)
x = np.linspace(0, 10, 10)
y = 2 * x + 1 + np.random.normal(size=x.size)

# R√©gression MCO
mco_model = LinearRegression()
mco_model.fit(x.reshape(-1, 1), y)
y_mco_pred = mco_model.predict(x.reshape(-1, 1))

# R√©gression Orthogonale
def orthogonal_func(beta, x):
    return beta[0] * x + beta[1]

data = RealData(x, y)
model = Model(orthogonal_func)
odr = ODR(data, model, beta0=[1., 2.])
odr_res = odr.run()
y_ortho_pred = orthogonal_func(odr_res.beta, x)

# Visualisation
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_mco_pred, label='MCO', color='red')
plt.plot(x, y_ortho_pred, label='R√©gression Orthogonale', color='green')
plt.legend()
plt.title('Regression MCO et Orthogonale')
plt.show()
```

> A travers cette visualisation, on peut voir que les droites de r√©gression des 2 m√©thodes sont quasiement surperpos√©es. En d'autres termes, les r√©sultats d'estimation fournis par les 2 m√©thodes sont assez proches.

2.  Les mod√®les qui d√©crivent une **relation non lin√©aire** entre la variable √† expliquer et la/les variable(s) explicative(s).

Il s'agit de la famille des m√©thodes de **R√©gressions non param√©triques** : La r√©gression non param√©trique est une technique statistique qui permet de mod√©liser les relations entre variables sans supposer de forme param√©trique sp√©cifique pour la relation. Contrairement √† la r√©gression param√©trique (comme la r√©gression lin√©aire), la r√©gression non param√©trique ne n√©cessite pas d'hypoth√®ses strictes sur la forme de la relation, ce qui permet de capturer des relations plus complexes et non lin√©aires. Voici quelques exemples de m√©thodes d'estimation non param√©triques :

-   **R√©gression par noyau** : La r√©gression par noyau est une m√©thode non param√©trique qui utilise une fonction noyau pour estimer la relation entre les variables ind√©pendantes et d√©pendantes. Une forme courante de r√©gression par noyau est la r√©gression par noyau de [Nadaraya-Watson](https://en.wikipedia.org/wiki/Kernel_regression). Cette m√©thode consiste √† estimer la valeur de la variable d√©pendante pour une valeur donn√©e de la variable ind√©pendante en prenant une moyenne pond√©r√©e des valeurs observ√©es, les poids √©tant d√©termin√©s par une fonction noyau.

::: {.callout-tip icon="false" collapse="true"}
## NB

1.  La r√©gression √† noyau est une extension de la r√©gression lin√©aire qui utilise des fonctions de noyau pour permettre la mod√©lisation de relations non lin√©aires. Une fonction de noyau $K(x, x')$ mesure la similarit√© entre deux points dans l'espace de caract√©ristiques.

## 2. Fonction de Co√ªt

La fonction de co√ªt pour la r√©gression √† noyau est souvent bas√©e sur la perte de Huber, la perte d'epsilon-insensible ou d'autres fonctions de perte robustes. Une fonction de co√ªt typique pour la r√©gression √† noyau est la suivante :

$J(\alpha) = \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j K(x_i, x_j) - \sum_{i=1}^{N} \alpha_i y_i$ $+$ $C \sum_{i=1}^{N} \max(0, |f(x_i) - y_i| - \epsilon)$

o√π : - $\alpha_i$ sont les coefficients duals. - $K(x_i, x_j)$est la fonction de noyau. - $y_i$ sont les valeurs cibles. - $C$est un param√®tre de r√©gularisation. - $\epsilon$ est un param√®tre de tol√©rance pour la perte epsilon-insensible.

## 3. Minimisation de la Fonction de Co√ªt

Pour minimiser cette fonction de co√ªt, nous utilisons des techniques d'optimisation convexes telles que le **Gradient Descent**, le **Coordinate Descent** ou des algorithmes sp√©cialis√©s comme le **Sequential Minimal Optimization (SMO)**.

#### Gradient Descent

L'algorithme de gradient descent ajuste les param√®tres $\alpha$ en suivant le gradient de la fonction de co√ªt :

$\alpha_i \leftarrow \alpha_i - \eta \frac{\partial J(\alpha)}{\partial \alpha_i}$

o√π $\eta$ est le taux d'apprentissage.

#### Sequential Minimal Optimization (SMO)

L'algorithme SMO divise le probl√®me d'optimisation en sous-probl√®mes plus petits qui peuvent √™tre r√©solus analytiquement. Ce processus est it√©ratif et continue jusqu'√† ce que la convergence soit atteinte.
:::

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity

# G√©n√©ration de donn√©es
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(x).ravel()

# Ajout de bruit
y[::5] += 3 * (0.5 - np.random.rand(16))

# R√©gression par noyau
from statsmodels.nonparametric.kernel_regression import KernelReg
kr = KernelReg(y, x, 'c')
y_pred, _ = kr.fit(x)

# Visualisation
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_pred, color='red', label='R√©gression par Noyau')
plt.legend()
plt.title('R√©gression par noyau')
plt.show()
plt.close()

```

-   **k-NN (k-plus proches voisin)** : Le k-plus proches voisins (k-NN) est un algorithme d'apprentissage supervis√© utilis√© pour la classification et la r√©gression. L'id√©e de base est de pr√©dire la classe ou la valeur d'une observation en utilisant les $k$ observations les plus proches dans l'ensemble d'entra√Ænement.

::: {.callout-tip icon="false" collapse="true"}
## NB

La minimisation de la fonction de co√ªt pour les m√©thodes de **k-plus proches voisins** (K-Nearest Neighbors ou K-NN) diff√®re des m√©thodes de r√©gression classiques comme celles utilisant des fonctions de noyau. **k-NN** est un algorithme non param√©trique qui ne construit pas de mod√®le explicite, mais utilise les donn√©es d'apprentissage pour pr√©dire les nouvelles instances directement.

### 1. Introduction √† K-NN

K-NN est une m√©thode bas√©e sur la proximit√© : pour une nouvelle observation, l'algorithme recherche les $k$ observations les plus proches dans l'ensemble de donn√©es d'apprentissage et effectue des pr√©dictions bas√©es sur ces voisins. Pour la r√©gression, la pr√©diction est g√©n√©ralement la moyenne des valeurs cibles des $k$ plus proches voisins.

### 2. Fonction de Co√ªt

Dans le contexte de K-NN, la fonction de co√ªt peut √™tre d√©finie comme l'erreur de pr√©diction globale, typiquement mesur√©e par l'erreur quadratique moyenne (Mean Squared Error, MSE) ou l'erreur absolue moyenne (Mean Absolute Error, MAE). Pour la minimiser, l'algorithme K-NN cherche √† optimiser les choix des hyperparam√®tres (notamment $k$ et la m√©trique de distance).

#### Erreur Quadratique Moyenne (MSE)

$J = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$

o√π $y_i$ est la valeur r√©elle et $\hat{y}_i$ est la valeur pr√©dite.

#### Erreur Absolue Moyenne (MAE)

$J = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$

### 3. Optimisation des Hyperparam√®tres

L'optimisation dans K-NN consiste principalement √† trouver le meilleur nombre de voisins ($k$) et la meilleure m√©trique de distance. Les approches courantes pour ce faire incluent la validation crois√©e et les grilles de recherche.

#### Validation Crois√©e

La validation crois√©e permet de diviser les donn√©es en plusieurs sous-ensembles (folds), en utilisant certains pour entra√Æner le mod√®le et les autres pour le tester, et en r√©p√©tant ce processus plusieurs fois pour r√©duire la variance des r√©sultats.

#### Grille de Recherche (Grid Search)

La grille de recherche explore syst√©matiquement une plage d'hyperparam√®tres pour trouver la combinaison qui minimise la fonction de co√ªt.

:::

```{python, echo=FALSE}
from sklearn.neighbors import KNeighborsRegressor

# R√©gression KNN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(x, y)
y_knn_pred = knn.predict(x)

# Visualisation
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_knn_pred, color='green', label='R√©gression KNN')
plt.legend()
plt.title('k-NN')
plt.show()

```

-   **Mod√®les Additifs G√©n√©ralis√©s (GAM)**: Les mod√®les additifs g√©n√©ralis√©s (GAM) sont une extension des mod√®les lin√©aires g√©n√©ralis√©s (GLM) qui permettent de mod√©liser des relations non lin√©aires. Les GAM utilisent des fonctions lisses (comme les splines) pour mod√©liser la relation entre chaque pr√©dicteur et la variable r√©ponse.

::: {.callout-tip icon="false" collapse="true"}
## NB

Dans un GAM, la fonction de co√ªt est minimis√©e en ajustant les fonctions lisses pour chaque variable explicative.

Un GAM a la forme suivante :

$g(E(y)) = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$

o√π $g$ est une fonction de lien, $E(y)$ est l'esp√©rance de $y$, $\beta_0$ est une constante, et $f_i$ sont des fonctions lisses des variables explicatives $x_i$.

### 2. Fonction de Co√ªt

La fonction de co√ªt pour les GAMs peut √™tre formul√©e en termes de log-vraisemblance n√©gative pour des distributions sp√©cifiques (par exemple, gaussienne, binomiale, poisson). En plus de la log-vraisemblance, on ajoute souvent des p√©nalit√©s de lissage pour √©viter le surajustement. La fonction de co√ªt typique est :

$J(f_1, f_2, \ldots, f_p) = -\text{log-vraisemblance} + \sum_{i=1}^{p} \lambda_i \int (f_i''(x_i))^2 dx_i$

o√π $\lambda_i$ sont des param√®tres de r√©gularisation pour les p√©nalit√©s de lissage.

### 3. Minimisation de la Fonction de Co√ªt

La minimisation de cette fonction de co√ªt se fait g√©n√©ralement par des m√©thodes d'optimisation num√©riques, telles que :

-   **Penalized Iteratively Reweighted Least Squares (P-IRLS)** : Une extension de l'algorithme IRLS utilis√© pour les mod√®les lin√©aires g√©n√©ralis√©s, adapt√©e pour inclure les p√©nalit√©s de lissage.
-   **Gradient Descent** et ses variantes.

:::

```{python, echo=FALSE}
#Splines et Mod√®les Additifs G√©n√©ralis√©s (GAM)

from patsy import dmatrix
from statsmodels.api import OLS

# G√©n√©ration de splines
x_basis = dmatrix("bs(x, df=6)", {"x": x}, return_type='dataframe')
model = OLS(y, x_basis).fit()
y_spline_pred = model.predict(x_basis)

# Visualisation
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_spline_pred, color='purple', label='Splines')
plt.legend()
plt.title('Mod√®le Additif G√©n√©ralis√©')
plt.show()

```

-   **For√™t al√©atoire**: Les for√™ts al√©atoires (Random Forests) sont une m√©thode d'apprentissage supervis√© populaire utilis√©e pour la classification et la r√©gression. Elles fonctionnent en construisant plusieurs arbres de d√©cision pendant la phase d'entra√Ænement et en produisant la classe qui est le mode des classes (classification) ou la moyenne des pr√©dictions (r√©gression) de chaque arbre individuel.

::: {.callout-tip icon="false" collapse="true"}
## NB

La for√™t al√©atoire (Random Forest) est un ensemble d'arbres de d√©cision entra√Æn√©s sur des sous-√©chantillons al√©atoires des donn√©es. L'id√©e est de combiner les pr√©dictions de plusieurs arbres pour am√©liorer la performance et r√©duire le risque de surajustement. La fonction de co√ªt dans une for√™t al√©atoire est g√©n√©ralement li√©e √† l'erreur de pr√©diction des arbres, et l'optimisation consiste √† ajuster divers hyperparam√®tres pour minimiser cette erreur.

### 1. Introduction √† la For√™t Al√©atoire

Une for√™t al√©atoire est un mod√®le d'ensemble qui construit plusieurs arbres de d√©cision et combine leurs pr√©dictions. Pour la r√©gression, la pr√©diction finale est la moyenne des pr√©dictions de tous les arbres.

### 2. Fonction de Co√ªt

Pour la r√©gression, la fonction de co√ªt est souvent mesur√©e par l'erreur quadratique moyenne (Mean Squared Error, MSE) ou l'erreur absolue moyenne (Mean Absolute Error, MAE) entre les valeurs r√©elles et les valeurs pr√©dites :

#### Erreur Quadratique Moyenne (MSE)

$J = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$

o√π $y_i$ est la valeur r√©elle et $\hat{y}_i$ est la valeur pr√©dite.

### 3. Optimisation des Hyperparam√®tres

La minimisation de la fonction de co√ªt dans une for√™t al√©atoire passe par l'optimisation des hyperparam√®tres suivants :

-   **Nombre d'arbres (n_estimators)** : Plus d'arbres peuvent am√©liorer la performance mais augmentent le temps de calcul.

-   **Profondeur maximale des arbres (max_depth)** : Limiter la profondeur peut pr√©venir le surajustement.

-   **Nombre de caract√©ristiques consid√©r√©es pour chaque split (max_features)** : Un bon r√©glage de ce param√®tre peut am√©liorer la performance.

-   **Taille minimale des √©chantillons pour diviser un n≈ìud (min_samples_split)**.

-   **Taille minimale des √©chantillons pour une feuille (min_samples_leaf)**.
:::

```{python, echo=FALSE}
from sklearn.ensemble import RandomForestRegressor

# R√©gression Random Forest
rf = RandomForestRegressor(n_estimators=100)
rf.fit(x, y)
y_rf_pred = rf.predict(x)

# Visualisation
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_rf_pred, color='orange', label='For√™t Al√©atoire')
plt.legend()
plt.title('For√™t al√©atoire')
plt.show()
```

# Comparaison des m√©thodes et conclusion

En conclusion, nous pouvons dire qu'il existe 2 grandes familles de m√©thodes de pr√©vision :

-   **Les mod√®les lin√©aires** : d√©crivant une relation lin√©aire entre la variable d√©pendante et les variables ind√©pendantes.
-   **Les mod√®les non lin√©aires** : inversement, d√©crivant une relation non lin√©aire entre la variable d√©pendante et les variables ind√©pendantes.

Gr√¢ce √† des repr√©sentations graphiques, nous allons visualiser les diff√©rences de r√©sultats entre ces deux familles de m√©thodes :

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor

# G√©n√©ration de donn√©es synth√©tiques
np.random.seed(0)
x = np.sort(5 * np.random.rand(80, 1), axis=0)
y = 2 * x**2 - 3 * x + 1 + np.random.normal(0, 1, x.shape)

# Mod√®le Lin√©aire Simple
lin_reg = LinearRegression()
lin_reg.fit(x, y)
y_lin_pred = lin_reg.predict(x)

# Mod√®le Polynomial (de degr√© 2)
poly = PolynomialFeatures(degree=2)
x_poly = poly.fit_transform(x)
poly_reg = LinearRegression()
poly_reg.fit(x_poly, y)
y_poly_pred = poly_reg.predict(x_poly)

# Mod√®le par For√™t Al√©atoire
rf = RandomForestRegressor(n_estimators=100)
rf.fit(x, y.ravel())
y_rf_pred = rf.predict(x)

# Visualisation des r√©sultats
plt.scatter(x, y, label='Donn√©es')
plt.plot(x, y_lin_pred, label='Mod√®le Lin√©aire', color='red')
plt.plot(x, y_poly_pred, label='Mod√®le Polynomial (degr√© 2)', color='green')
plt.plot(x, y_rf_pred, label='For√™t Al√©atoire', color='orange')
plt.legend()
plt.show()
```

Par ailleurs, pour chacune de ces m√©thodes nous avons mis en √©vidence les √©l√©ments suivants :

-   **La forme fonctionnelle du mod√®le** : La forme fonctionnelle d'un mod√®le se r√©f√®re √† la sp√©cification de la relation math√©matiques entre les variables ind√©pendantes (ou explicatives) et la variable d√©pendante (ou √† expliquer). Cette sp√©cification d√©finit comment les variables sont li√©es dans le mod√®le et peut prendre plusieurs formes selon la nature des donn√©es et les hypoth√®ses sous-jacentes

-   **Les principales hypoth√®ses associ√©es au mod√®le** : Les principales hypoth√®ses d'un mod√®le sont essentielles pour garantir la validit√© et la fiabilit√© des estimations des coefficients.

-   Leur application dans des outils techniques appropri√©s : {{< fa brands r-project >}} et `Python` {{< fa brands python >}}

# Outils techniques

Pour r√©aliser ces travaux, nous avons utilis√© {{< fa brands r-project >}} et `Python` {{<fa brands python >}} pour la partie mod√©lisation. Pour la mise en forme du rapport ainsi que du support de pr√©sentation, nous avons utilis√© [LateX](https://fr.wikipedia.org/wiki/LaTeX).

::: {style="text-align:center;"}
> Vous trouverez le rapport ainsi que le support de pr√©sentation ici :

::: chevron-styling
{{< iconify pixelarticons:chevron-down size=5x >}}
:::
:::

::: {style="text-align:center;"}
[Rapport : Repr√©sentation sous forme de diagramme des MCO {{< fa arrow-up-right-from-square >}}](/assets/img/Projet1/AlternativesMCO.pdf){.btn .btn-outline-primary .btn .center role="button"}
:::

::: {style="text-align:center;"}
[Pr√©sentation : Alternatives aux MCO {{< fa arrow-up-right-from-square >}}](/assets/img/Projet1/Alternatives%20aux%20MCO.pdf){.btn .btn-outline-primary .btn .center role="button"}
:::
